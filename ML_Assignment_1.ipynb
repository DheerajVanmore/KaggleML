{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qqwhiAeQ9-D",
        "outputId": "4b8f04fe-58ff-4e3f-96e3-14a0c7b92e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task1_cancer_classification.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile task1_cancer_classification.py\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Utility: Metrics Printer\n",
        "# -----------------------------\n",
        "def evaluate(model, X_train, y_train, X_test, y_test, name):\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    train_error = 1 - accuracy_score(y_train, y_train_pred)\n",
        "    test_error = 1 - accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    prec = precision_score(y_test, y_test_pred)\n",
        "    rec = recall_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test, y_test_pred)\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(f\"Train Error: {train_error:.4f}\")\n",
        "    print(f\"Test Error : {test_error:.4f}\")\n",
        "    print(f\"Accuracy  : {acc:.4f}\")\n",
        "    print(f\"Precision : {prec:.4f}\")\n",
        "    print(f\"Recall    : {rec:.4f}\")\n",
        "    print(f\"F1-score  : {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    return train_error, test_error\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Main\n",
        "# -----------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", required=True, help=\"Path to data.csv\")\n",
        "    parser.add_argument(\"--test_size\", type=float, default=0.2)\n",
        "    parser.add_argument(\"--model\", choices=[\"logistic\", \"tree\", \"both\"], default=\"both\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Dataset has no headers\n",
        "    df = pd.read_csv(args.data, header=None)\n",
        "\n",
        "    # Column 1 is label: M or B\n",
        "    y = df[1].map({\"M\": 1, \"B\": 0})\n",
        "    X = df.drop(columns=[0, 1])  # drop ID and label\n",
        "\n",
        "    # Train-Test Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=args.test_size, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Scaling (important for Logistic Regression)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    conclusions = []\n",
        "\n",
        "    # ---------------- Logistic Regression ----------------\n",
        "    if args.model in [\"logistic\", \"both\"]:\n",
        "        logreg = LogisticRegression(max_iter=500)\n",
        "        logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "        tr_err, te_err = evaluate(\n",
        "            logreg,\n",
        "            X_train_scaled, y_train,\n",
        "            X_test_scaled, y_test,\n",
        "            \"Logistic Regression\"\n",
        "        )\n",
        "\n",
        "        if tr_err < te_err:\n",
        "            conclusions.append(\"Logistic Regression shows slight overfitting.\")\n",
        "        else:\n",
        "            conclusions.append(\"Logistic Regression generalizes well with similar train and test error.\")\n",
        "\n",
        "    # ---------------- Decision Tree ----------------\n",
        "    if args.model in [\"tree\", \"both\"]:\n",
        "        tree = DecisionTreeClassifier(random_state=42)\n",
        "        tree.fit(X_train, y_train)\n",
        "\n",
        "        tr_err, te_err = evaluate(\n",
        "            tree,\n",
        "            X_train, y_train,\n",
        "            X_test, y_test,\n",
        "            \"Decision Tree\"\n",
        "        )\n",
        "\n",
        "        if tr_err < te_err:\n",
        "            conclusions.append(\"Decision Tree shows strong overfitting (very low train error, higher test error).\")\n",
        "        elif tr_err > te_err:\n",
        "            conclusions.append(\"Decision Tree may be underfitting.\")\n",
        "        else:\n",
        "            conclusions.append(\"Decision Tree has moderate generalization.\")\n",
        "\n",
        "    # ---------------- Final Conclusion ----------------\n",
        "    print(\"\\n===== Conclusion =====\")\n",
        "    print(\n",
        "        \"The Logistic Regression model benefits from feature scaling and typically shows \"\n",
        "        \"similar training and test error, indicating good generalization and low variance. \"\n",
        "        \"It is a linear model, so it may slightly underfit complex patterns but remains stable.\"\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"\\nDecision Trees can fit training data very closely, often achieving near-zero training error. \"\n",
        "        \"If test error is much higher, this indicates overfitting due to high model variance. \"\n",
        "        \"This happens because trees memorize noise and small patterns in the training data.\"\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        \"\\nRelevant ML issues in this problem:\\n\"\n",
        "        \"1. Feature scaling: Logistic Regression relies on distance-based optimization, so unscaled features can \"\n",
        "        \"cause slow convergence and poor coefficients.\\n\"\n",
        "        \"2. Feature correlation: Many tumor measurements are highly correlated, which can affect linear models.\\n\"\n",
        "        \"3. Data leakage risk: Scaling must be fit only on training data, not entire dataset.\\n\"\n",
        "        \"4. Class imbalance: Benign cases are more than malignant, so accuracy alone can be misleading; \"\n",
        "        \"precision and recall are important.\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python task1_cancer_classification.py --data wdbc.data --test_size 0.2 --model both\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OhGXivRRYNv",
        "outputId": "8a35ba85-a86c-42d1-825b-a15119270b95"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Logistic Regression =====\n",
            "Train Error: 0.0132\n",
            "Test Error : 0.0351\n",
            "Accuracy  : 0.9649\n",
            "Precision : 0.9750\n",
            "Recall    : 0.9286\n",
            "F1-score  : 0.9512\n",
            "Confusion Matrix:\n",
            "[[71  1]\n",
            " [ 3 39]]\n",
            "\n",
            "===== Decision Tree =====\n",
            "Train Error: 0.0000\n",
            "Test Error : 0.0702\n",
            "Accuracy  : 0.9298\n",
            "Precision : 0.9048\n",
            "Recall    : 0.9048\n",
            "F1-score  : 0.9048\n",
            "Confusion Matrix:\n",
            "[[68  4]\n",
            " [ 4 38]]\n",
            "\n",
            "===== Conclusion =====\n",
            "The Logistic Regression model benefits from feature scaling and typically shows similar training and test error, indicating good generalization and low variance. It is a linear model, so it may slightly underfit complex patterns but remains stable.\n",
            "\n",
            "Decision Trees can fit training data very closely, often achieving near-zero training error. If test error is much higher, this indicates overfitting due to high model variance. This happens because trees memorize noise and small patterns in the training data.\n",
            "\n",
            "Relevant ML issues in this problem:\n",
            "1. Feature scaling: Logistic Regression relies on distance-based optimization, so unscaled features can cause slow convergence and poor coefficients.\n",
            "2. Feature correlation: Many tumor measurements are highly correlated, which can affect linear models.\n",
            "3. Data leakage risk: Scaling must be fit only on training data, not entire dataset.\n",
            "4. Class imbalance: Benign cases are more than malignant, so accuracy alone can be misleading; precision and recall are important.\n"
          ]
        }
      ]
    }
  ]
}